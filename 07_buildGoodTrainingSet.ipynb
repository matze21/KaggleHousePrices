{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd191aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0907030",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "uselessFeatures = ['Utilities', 'Street', 'Heating', 'Condition2', 'MiscFeature',\n",
    "       'MiscVal', 'LowQualFinSF', 'PoolArea', 'BsmtHalfBath',\n",
    "       'GarageCond', '3SsnPorch', 'PoolQC', 'Electrical', 'GarageQual',\n",
    "       'PavedDrive', 'RoofMatl', 'Id']\n",
    "\n",
    "def addNewFeaturesToDF(concatDropped):\n",
    "    concatDropped['YearsSinceRemodeled']  = concatDropped['YrSold'] - concatDropped['YearRemodAdd']\n",
    "    concatDropped['YearsOld']  = concatDropped['YrSold'] - concatDropped['YearBuilt']\n",
    "    concatDropped['TotalSF']  = concatDropped['TotalBsmtSF'] + concatDropped['1stFlrSF'] + concatDropped['2ndFlrSF'] + concatDropped['1stFlrSF']\n",
    "    concatDropped['UnfinishedSF']  = (concatDropped['LowQualFinSF'] + concatDropped['BsmtUnfSF']) / concatDropped['TotalSF']\n",
    "    concatDropped['FinishedSFBasement']  = 1 - (concatDropped['BsmtUnfSF']) / concatDropped['TotalBsmtSF']\n",
    "    concatDropped['TotalOutsideSF']  = concatDropped['WoodDeckSF'] + concatDropped['OpenPorchSF'] + concatDropped['EnclosedPorch'] + concatDropped['3SsnPorch']+ concatDropped['ScreenPorch'] + concatDropped['PoolArea']\n",
    "    concatDropped['PropertySF']  = concatDropped['TotalSF'] + concatDropped['TotalOutsideSF'] + concatDropped['MasVnrArea'] + concatDropped['LotFrontage']\n",
    "    concatDropped['grassArea'] = concatDropped['PropertySF']  - concatDropped['LotArea'] \n",
    "    concatDropped['bathrooms'] = concatDropped['BsmtFullBath']  + concatDropped['BsmtHalfBath'] +concatDropped['FullBath']  + concatDropped['HalfBath'] \n",
    "    concatDropped['numberBedAndBath'] = concatDropped['BedroomAbvGr']  + concatDropped['FullBath'] \n",
    "    \n",
    "\n",
    "def encodeEntireData(X_train):\n",
    "    addNewFeaturesToDF(X_train)\n",
    "    X_train = X_train.drop(uselessFeatures, axis = 1)\n",
    "    cache = []\n",
    "    for feature in X_train.columns.values:\n",
    "        if feature == 'Type':\n",
    "            continue\n",
    "        else:\n",
    "            label_encoder =LabelEncoder()\n",
    "            X_train[feature]= label_encoder.fit_transform(X_train[feature])\n",
    "            cache.append(label_encoder)\n",
    "            b = np.reshape(X_train[feature].to_numpy(), (X_train.shape[0],1))\n",
    "            b = b / max(abs(b))\n",
    "            a = b - np.mean(b)\n",
    "            #a = Normalizer().fit_transform(b)\n",
    "            X_train[feature] = a\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddeb70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData = pd.read_csv(\"train.csv\")\n",
    "trainingData['Type'] = 'train'\n",
    "testData = pd.read_csv(\"test.csv\")\n",
    "testData['Type'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400a25a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = pd.concat([trainingData, testData], axis = 0)\n",
    "concat = concat.drop('SalePrice', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b220196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatProcessed = encodeEntireData(concat)\n",
    "\n",
    "X_train = concatProcessed.loc[concatProcessed.Type == 'train']\n",
    "X_train = X_train.drop('Type', axis = 1)\n",
    "Y_train = trainingData.SalePrice\n",
    "X_val = concatProcessed.loc[concatProcessed.Type == 'test']\n",
    "\n",
    "corrMatrix = concatProcessed.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414fdb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be580685",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "corrMatrix['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c2fd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(concat2[['Id', 'SalePrice']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d9eb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#automatic outlier detection\n",
    "from sklearn.ensemble import IsolationForest\n",
    "clf = IsolationForest(n_estimators = 10)\n",
    "clf.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179a4171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "for i in X_train.columns.values:\n",
    "    fig, (ax1, ax2, ax3)=plt.subplots(1,3)\n",
    "    #print(i)\n",
    "    ax1.set_title(i)\n",
    "    ax1.hist(X_train[i], bins = np.linspace(min(X_train[i]), max(X_train[i]), 100))\n",
    "\n",
    "    ax3.scatter(Y_train, X_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967edb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test, Y_train2, Y_test = train_test_split(X_train, Y_train, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b22785",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = RandomForestRegressor(n_estimators = 1000)\n",
    "clf2.fit(X_train2, Y_train2)\n",
    "\n",
    "print(clf2.score(X_train2,Y_train2))\n",
    "print(clf2.score(X_test,Y_test))\n",
    "y = clf2.feature_importances_\n",
    "x = np.linspace(0, len(y)-1, len(y))\n",
    "fig=plt.figure()\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b604d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = clf2.predict(X_test)\n",
    "np.sum(abs(np.divide(Y_test - Y_pred, Y_test)))/len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1fdb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([X_train.columns.values, y]).T\n",
    "sortedArr = data[data[:,1].argsort()]\n",
    "print(sortedArr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f65137d",
   "metadata": {},
   "source": [
    "## predict log of Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9041d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(np.log(Y_train2))\n",
    "a.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43def9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3 = RandomForestRegressor(n_estimators = 1000)\n",
    "clf3.fit(X_train2, np.log(Y_train2))\n",
    "\n",
    "print(clf3.score(X_train2,np.log(Y_train2)))\n",
    "print(clf3.score(X_test,np.log(Y_test)))\n",
    "y = clf3.feature_importances_\n",
    "x = np.linspace(0, len(y)-1, len(y))\n",
    "fig=plt.figure()\n",
    "plt.plot(x,y)\n",
    "\n",
    "Y_pred = np.exp(clf3.predict(X_test))\n",
    "print(np.sum(abs(np.divide(Y_test - Y_pred, Y_test)))/len(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d331ab",
   "metadata": {},
   "source": [
    "# Stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530004bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [#('rf1', RandomForestRegressor(n_estimators=10, random_state=42)),\n",
    "              #('rf2', RandomForestRegressor(n_estimators=100, random_state=42)),\n",
    "              #('rf3', RandomForestRegressor(n_estimators=19, random_state=42)),\n",
    "              #('rf4', RandomForestRegressor(n_estimators=4, random_state=42)),\n",
    "              ('rf5', RandomForestRegressor(n_estimators=300, max_leaf_nodes = 20, random_state=42)),\n",
    "              ('rf6', RandomForestRegressor(n_estimators=300, max_leaf_nodes = 25, bootstrap=False, random_state=42)),\n",
    "              ('rf7', RandomForestRegressor(n_estimators=100, max_leaf_nodes = 20, random_state=42)),\n",
    "              ('xgboost1', XGBRegressor(learning_rate=0.1, max_depth=10, random_state=42)),\n",
    "              ('xgboost2', XGBRegressor(learning_rate=0.1, max_depth=8, random_state=42)),\n",
    "              ('xgboost3', XGBRegressor(n_estimators=100, learning_rate=0.01, max_depth=6, random_state=42)),\n",
    "    \n",
    "              ('xgboost4', XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)),\n",
    "    \n",
    "              #('svr', LinearSVR(random_state=42)),\n",
    "              #('svr', make_pipeline(StandardScaler(),LinearSVR(random_state=42))),\n",
    "              ('nn1', MLPRegressor(hidden_layer_sizes = (1000,500,100,),random_state=42, max_iter=1000, early_stopping=True)),\n",
    "              #('nn2', MLPRegressor(hidden_layer_sizes = (100,),random_state=42, max_iter=500, early_stopping=True)),\n",
    "              #('nn3', MLPRegressor(hidden_layer_sizes = (10, 10,),random_state=42, max_iter=5000, early_stopping=True)),\n",
    "              #('logReg', LogisticRegression()),\n",
    "            # ('kmeans', make_pipeline(StandardScaler(),KMeans(random_state=42))),\n",
    "              \n",
    "             ]\n",
    "\n",
    "clf = StackingRegressor(\n",
    "     estimators=estimators#, final_estimator = LogisticRegression(random_state=42)\n",
    "    #final_estimator=RandomForestRegressor(n_estimators=100, random_state=42)\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484ee9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train2, Y_train2)\n",
    "print(clf.score(X_train2,Y_train2))\n",
    "print(clf.score(X_test,Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
